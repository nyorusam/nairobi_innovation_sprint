{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad52a5c",
   "metadata": {},
   "source": [
    "# Run evaluation on datasets and report metrics and predictions\n",
    "\n",
    "* going forward we will use this as our standard approach to obtain metrics for a model\n",
    "* currently, we are still using the default models (ie, whisper models as they are), but once we have trained our own models we will use this notebook to get the WER for the adapted models (which hopefully is much better than the current base models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e9a77",
   "metadata": {},
   "source": [
    "# Imports and defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785b5aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 26 21:00:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:00:07.0 Off |                    0 |\n",
      "| N/A   51C    P8             17W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import itables\n",
    "itables.init_notebook_mode()\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.pipelines.pt_utils import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient dataset handling\n",
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this is slower when more stored in dataset (eg features), as it leads to data being copied around\n",
    "class AudioTextDataset(Dataset):\n",
    "\n",
    "    DEFAULT_UTTERANCE_FIELDS = ['audio_id', 'speaker_id',  'language', 'locale', 'accents',\n",
    "                         'prompt_type', 'prompt_id',\n",
    "                         'transcription', 'audio_length', 'transcript_length']\n",
    "\n",
    "    def __init__(self, dataset: Dataset, utterance_fields=DEFAULT_UTTERANCE_FIELDS):\n",
    "        self.dataset = dataset\n",
    "        self.utterance_fields = utterance_fields\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # only select the fields if they are present in the dataset\n",
    "        result = {k: self.dataset[i][k] for k in self.utterance_fields if k in self.dataset[i]}\n",
    "        # these two fields are required for the ASR pipeline\n",
    "        result['sampling_rate'] = self.dataset[i]['audio']['sampling_rate']\n",
    "        result['raw'] = self.dataset[i]['audio']['array']\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load as metrics_loader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "wer_metric = metrics_loader(\"wer\")\n",
    "cer_metric = metrics_loader(\"cer\")\n",
    "\n",
    "transcript_normalizer = BasicTextNormalizer()\n",
    "\n",
    "def get_wer_cer(references, predictions,\n",
    "                calculate_utterance_level_averaged_wer=False,\n",
    "                normalize=True, verbose=True,\n",
    "                ):\n",
    "  # calculate_utterance_level_averaged_wer -- we first calculate the WER per\n",
    "  # utterance and then average. This is not the standard way to calculate WER\n",
    "  # on a corpus, but in a scenario of high WER (as for NSS) this allows to cap\n",
    "  # at 1.0 on a per-utterance level.\n",
    "  rs = references\n",
    "  ps = predictions\n",
    "  if normalize:\n",
    "    pred_strs = [transcript_normalizer(x) for x in predictions]\n",
    "    label_strs = [transcript_normalizer(x) for x in references]\n",
    "  if calculate_utterance_level_averaged_wer:\n",
    "    wers = []\n",
    "    cers = []\n",
    "    for pred_str, label_str in zip(pred_strs, label_strs):\n",
    "      p = transcript_normalizer(pred_str)\n",
    "      l = transcript_normalizer(label_str)\n",
    "      wer = wer_metric.compute(predictions=[p], references=[l])\n",
    "      cer = cer_metric.compute(predictions=[p], references=[l])\n",
    "      wers.append(wer)\n",
    "      cers.append(cer)\n",
    "      if verbose:\n",
    "        print(label_str, '-->', pred_str, '-->', wer, cer)\n",
    "    wer = np.mean([min(1.0, x) for x in wers])\n",
    "    cer = np.mean([min(1.0, x) for x in cers])\n",
    "  else:\n",
    "    wer =  min(1, wer_metric.compute(references=label_strs, predictions=pred_strs))\n",
    "    cer =  min(1, cer_metric.compute(references=label_strs, predictions=pred_strs))\n",
    "\n",
    "    if verbose:\n",
    "      for pred_str, label_str in zip(pred_strs, label_strs):\n",
    "        print(label_str, '-->', pred_str)\n",
    "\n",
    "  return (wer, cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf78c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    def clean_res(row):\n",
    "        for f in df.columns:\n",
    "            v = row[f]\n",
    "            if isinstance(v, list):\n",
    "                row[f] = v[0]\n",
    "        return row\n",
    "    df = df.apply(clean_res, axis=1)\n",
    "    df['prediction'] = df['text'].str.strip() if isinstance(df['text'], str) else df['text']\n",
    "   \n",
    "    df['ground_truth'] = df['transcription'].str.strip() if isinstance(df['transcription'], str) else df['transcription']\n",
    "    df = df.drop(columns=['text', 'transcription'])\n",
    "    return df\n",
    "\n",
    "def add_speaker_metadata(results_df, metadata_df):\n",
    "    # Merge metadata with results_df on 'speaker_id'\n",
    "    merged_df = results_df.merge(metadata_df, on='speaker_id', how='left')\n",
    "\n",
    "    # Check if any speaker_id in results_df is missing in metadata_df\n",
    "    missing_speakers = set(results_df['speaker_id']) - set(metadata_df['speaker_id'])\n",
    "    if missing_speakers:\n",
    "        print(f\"Warning: Missing metadata for speakers: {missing_speakers}\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def calculate_error_rates(results_df, verbose=False):\n",
    "\n",
    "  def get_row_wer(row):\n",
    "    reference = transcript_normalizer(row['ground_truth'])\n",
    "    prediction = transcript_normalizer(row['prediction'])\n",
    "    return get_wer_cer(references=[reference], predictions=[prediction], normalize=True, verbose=verbose)[0]\n",
    "\n",
    "  def get_row_cer(row):\n",
    "    reference = transcript_normalizer(row['ground_truth'])\n",
    "    prediction = transcript_normalizer(row['prediction'])\n",
    "    return get_wer_cer(references=[reference], predictions=[prediction], normalize=True, verbose=verbose)[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # add normalized ground truth and prediction\n",
    "  results_df['ground_truth_normalized'] = results_df['ground_truth'].apply(lambda x: transcript_normalizer(x))\n",
    "  results_df['prediction_normalized'] = results_df['prediction'].apply(lambda x: transcript_normalizer(x))\n",
    "\n",
    "  results_df['wer'] = results_df.apply(get_row_wer, axis=1)\n",
    "  results_df['cer'] = results_df.apply(get_row_cer, axis=1)\n",
    "\n",
    "  overall_wer_cer = get_wer_cer(references=results_df.ground_truth.tolist(),\n",
    "                                predictions=results_df.prediction.tolist(),\n",
    "                                calculate_utterance_level_averaged_wer=False,\n",
    "                                normalize=True, verbose=verbose)\n",
    "\n",
    "  avg_utterance_level_wer_cer = get_wer_cer(references=results_df.ground_truth.tolist(),\n",
    "                                predictions=results_df.prediction.tolist(),\n",
    "                                calculate_utterance_level_averaged_wer=True,\n",
    "                                normalize=True, verbose=verbose)\n",
    "\n",
    "\n",
    "  print('Overall WER (normalized):', round(overall_wer_cer[0],3))\n",
    "  print('Overall CER (normalized):', round(overall_wer_cer[1],3))\n",
    "  print('Avg WER (normalized):', round(avg_utterance_level_wer_cer[0],3))\n",
    "  print('Avg CER (normalized):', round(avg_utterance_level_wer_cer[1],3))\n",
    "\n",
    "  return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, limit_to_30_seconds=True):\n",
    "    \"\"\"\n",
    "    Load a dataset from Hugging Face Hub.\n",
    "    If limit_to_30_seconds is True, will only load examples with audio length <= 30 seconds.\n",
    "    \"\"\"\n",
    "    ds = datasets.load_dataset(dataset_name, split='test', streaming=False)\n",
    "    orig_len = len(ds)\n",
    "    if limit_to_30_seconds:\n",
    "        ds = ds.filter(lambda example: example['audio_length'] <= 30)\n",
    "        print(f\"Filtered dataset from {orig_len} to {len(ds)} examples with audio length <= 30 seconds\")\n",
    "    return ds\n",
    "\n",
    "def load_speaker_metadata(dataset_name):\n",
    "    \"\"\"\n",
    "    Load speaker metadata from a dataset.\n",
    "    \"\"\"\n",
    "    metadata_file_path = hf_hub_download(\n",
    "        repo_id=dataset_name,\n",
    "        filename=\"speaker_metadata.csv\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    # print(f\"File downloaded to: {metadata_file_path}\")\n",
    "\n",
    "    metadata_df = pd.read_csv(metadata_file_path)\n",
    "\n",
    "    # map severity\n",
    "    if 'severity_speech_impairment' in metadata_df.columns:\n",
    "        metadata_df['severity'] = metadata_df['severity_speech_impairment'].apply(lambda x: x.split()[0].lower())\n",
    "        metadata_df = metadata_df.drop(columns='severity_speech_impairment')\n",
    "  \n",
    "    \n",
    "\n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the kernel is running remotely, the file is stored remotely as well\n",
    "# this code helps to download it\n",
    "\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "\n",
    "def create_download_link(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = base64.b64encode(f.read()).decode()\n",
    "    \n",
    "    html = f'<a href=\"data:application/octet-stream;base64,{data}\" download=\"{filename}\">📥 Download {filename}</a>'\n",
    "    display(HTML(html))\n",
    "\n",
    "# # Usage\n",
    "# create_download_link(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ba8a2",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993456ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHISPER_MODEL_NAME = \"openai/whisper-tiny\"\n",
    "# WHISPER_MODEL_NAME = \"openai/whisper-small\"\n",
    "WHISPER_MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "\n",
    "DATASET_NAME = \"cdli/kenyan_english_nonstandard_speech_v0\"\n",
    "LANGUAGE = \"en\"\n",
    "LENGTH_LIMIT = True\n",
    "\n",
    "# DATASET_NAME = \"cdli/kenyan_swahili_nonstandard_speech_v0\"\n",
    "# LANGUAGE = \"sw\"\n",
    "# LENGTH_LIMIT = True\n",
    "\n",
    "# DATASET_NAME = \"cdli/common_voice_swahili_small\"\n",
    "# LANGUAGE = \"sw\"\n",
    "# LENGTH_LIMIT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e46351",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721eb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to login to Hugging Face Hub to access the datasets\n",
    "# use your huggingface token (you can get that under \"Access Tokens\" in your HuggingFace account)\n",
    "\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = input()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35169197",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME, limit_to_30_seconds=LENGTH_LIMIT)\n",
    "print(f\"Loaded dataset with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = load_speaker_metadata(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8fff6",
   "metadata": {},
   "source": [
    "# Load model and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b89eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Note: if handling data with more than 30 seconds, you need to set return_timestamps=True\n",
    "# That will require batch size of 1 and also slow down the inference significantly due to timestamping\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", \n",
    "                model=WHISPER_MODEL_NAME,\n",
    "                #return_timestamps=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2a7c3",
   "metadata": {},
   "source": [
    "# Run inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec90bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs={\n",
    "    \"language\": LANGUAGE, \n",
    "    \"task\": \"transcribe\",\n",
    "    \"max_length\": 448, # Note: don't exceed 448 - otherwise you'll get index errors when max_length exceeds the models positional encoding limits\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False\n",
    "}    \n",
    "print(f\"Using model: {pipe.model.name_or_path} with language: {LANGUAGE}\")\n",
    "N = len(dataset)\n",
    "# N = 100\n",
    "print(f\"Number of examples in dataset: {N}\")\n",
    "\n",
    "results = []\n",
    "for out in tqdm(pipe(AudioTextDataset(dataset.take(N)), batch_size=32, generate_kwargs=generate_kwargs), total=N):\n",
    "    # print(out)\n",
    "    results.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1a646",
   "metadata": {},
   "source": [
    "## Get overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing results...\n",
    "\n",
    "print(\"Finalizing results...\")\n",
    "results_df = prepare_results(results)\n",
    "print(\"Getting speaker metadata...\")\n",
    "results_df = add_speaker_metadata(results_df, metadata_df)\n",
    "print(f\"Calculating WER and CER for {len(results_df)} examples...\")\n",
    "results_df = calculate_error_rates(results_df, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ee8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=120\n",
    "def truncate_text(text, max_length):\n",
    "    if len(str(text)) > max_length:\n",
    "        return str(text)[:max_length] + \"...\"\n",
    "    return str(text)\n",
    "df_display = results_df.copy()\n",
    "df_display['prediction'] = df_display['prediction'].apply(\n",
    "    lambda x: truncate_text(x, max_len)\n",
    ")\n",
    "df_display['ground_truth'] = df_display['ground_truth'].apply(\n",
    "    lambda x: truncate_text(x, max_len)\n",
    ")\n",
    "df_display = df_display.drop(columns=['ground_truth_normalized', 'prediction_normalized'])\n",
    "itables.show(df_display.round(2), connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3fae0",
   "metadata": {},
   "source": [
    "## Get aggregated results\n",
    "\n",
    "* we aggregate on the per speaker level first, and then can further aggregate by certain aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_speaker_agg = results_df[['speaker_id', 'severity', 'etiology', 'wer', 'cer']].groupby(['speaker_id', 'severity', 'etiology']).agg(['mean', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27587b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-speaker results\n",
    "itables.show(per_speaker_agg.sort_values(by=['severity','speaker_id']).round(2), connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-severity results\n",
    "itables.show(per_speaker_agg.groupby(['severity']).agg(['mean', 'count']).round(2), connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9833e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-etiology results\n",
    "itables.show(per_speaker_agg.groupby(['etiology']).agg(['mean', 'count']).round(2), connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c168b1",
   "metadata": {},
   "source": [
    "# Safe Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7320215",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_OUTPUT_FOLDER = '/tmp/predictions'\n",
    "if not os.path.exists(LOCAL_OUTPUT_FOLDER):\n",
    "    os.makedirs(LOCAL_OUTPUT_FOLDER)\n",
    "print('saving predictions to folder: ', LOCAL_OUTPUT_FOLDER)\n",
    "\n",
    "output_filename = os.path.join(LOCAL_OUTPUT_FOLDER, DATASET_NAME.replace('cdli/', '') + '_' + WHISPER_MODEL_NAME.replace('openai/','') + '.tsv')\n",
    "print('local output filename:', output_filename)\n",
    "results_df.to_csv(output_filename, index=False, sep='\\t', encoding='utf-8') \n",
    "\n",
    "\n",
    "# download locally\n",
    "create_download_link(output_filename)\n",
    "print(\"Click on this link to start download...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
